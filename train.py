# aorpo/train.py
from __future__ import annotations
import os
import jax
import jax.numpy as jnp
import hydra
from omegaconf import DictConfig, OmegaConf
from tqdm import tqdm

# ===== ä½ é¡¹ç›®é‡Œçš„æ¨¡å— =====
from aorpo.utils.replay import ReplayBuffer
from aorpo.rollout.collect import collect_real_data
from aorpo.rollout.rollout import rollout_model, compute_rollout_lengths

from aorpo.agents.policy import init_policy_model, PolicyNet
from aorpo.agents.q_function import init_q_function   # ä½ åœ¨ q_function.py é‡Œæä¾›çš„åˆå§‹åŒ–å‡½æ•°
from aorpo.agents.update_q_function import update_q_function
from aorpo.agents.update_policy import update_policy

from aorpo.agents.model_dynamics import (
    init_model,
    train_step as model_train_step,
    Standardizer,
)


# -------------------------------------------------
# è¾…åŠ©ï¼šè½¯æ›´æ–° target Q
# -------------------------------------------------
def soft_update(target_state, source_state, tau: float):
    new_params = jax.tree_util.tree_map(
        lambda t, s: (1.0 - tau) * t + tau * s, target_state.params, source_state.params
    )
    return target_state.replace(params=new_params)


# -------------------------------------------------
# è¾…åŠ©ï¼šæŠŠä¸€æ‰¹ dict(jnp arrays) åŠ å…¥ replay
# -------------------------------------------------
def add_batch_to_replay(replay: ReplayBuffer, batch: dict) -> ReplayBuffer:
    return replay.add_batch(batch)


# -------------------------------------------------
# JAX é£æ ¼ policy / opponent çš„â€œå¯è°ƒç”¨å‡½æ•°â€ï¼ˆä¾› collect ä½¿ç”¨ï¼‰
#   collect_real_data(policy_fn, opp_fn, ...) æœŸæœ›ï¼š
#   - policy_fn(s, key) -> ego åŠ¨ä½œ a_i
#   - opp_fn(s, key)    -> æ‹¼å¥½çš„å¯¹æ‰‹åŠ¨ä½œå‘é‡ a_-i
# -------------------------------------------------
def make_policy_fn(policy_state):
    def policy_fn(obs, key):
        act, _, key = PolicyNet.sample_action(
            policy_state.params, policy_state.apply_fn, key, obs
        )
        return act
    return policy_fn


def make_opp_fn(opponent_states):
    """ç®€å•å®ç°ï¼šæŠŠæ¯ä¸ª opponent çš„åŠ¨ä½œæ‹¼æ¥èµ·æ¥ã€‚
       ç°åœ¨å¦‚æœä½ è¿˜æ²¡çœŸæ­£è®­ç»ƒ opponentï¼Œå°±å…ˆç»™ 0 åŠ¨ä½œå ä½ã€‚"""
    if not opponent_states:
        def zeros_opp(obs, key):
            # è®©å¯¹æ‰‹ç»´åº¦ä¸º 0 æ—¶è¿”å›ç©ºå¼ é‡ï¼ˆcollect ä¼šå¤„ç†ï¼‰
            return jnp.zeros((obs.shape[0], 0), dtype=obs.dtype)
        return zeros_opp

    def opp_fn(obs, key):
        acts = []
        k = key
        for state in opponent_states:
            a_j, _, k = PolicyNet.sample_action(state.params, state.apply_fn, k, obs)
            acts.append(a_j)
        return jnp.concatenate(acts, axis=-1)
    return opp_fn


# -------------------------------------------------
# ä¸»è®­ç»ƒæµç¨‹ï¼ˆHydraï¼‰
# -------------------------------------------------
@hydra.main(config_path="aorpo/configs", config_name="train", version_base=None)
def main(cfg: DictConfig):

    print("\n===== Config =====")
    print(OmegaConf.to_yaml(cfg))

    rng = jax.random.PRNGKey(cfg.seed)

    # ç»´åº¦
    obs_dim = cfg.env.obs_dim
    act_dim = cfg.env.act_dim
    opp_num = getattr(cfg.train, "num_opponents", 0)
    opp_dim = act_dim * opp_num  # ç®€å•å‡è®¾æ¯ä¸ªå¯¹æ‰‹åŠ¨ä½œç»´åº¦ä¸ ego ç›¸åŒ

    # --- Replay Buffers
    replay_env = ReplayBuffer.create(cfg.replay.capacity, obs_dim, act_dim, opp_dim)
    replay_model = ReplayBuffer.create(cfg.replay.capacity, obs_dim, act_dim, opp_dim)

    # --- åˆå§‹åŒ–ç½‘ç»œ
    rng, k1 = jax.random.split(rng)
    _, policy_state = init_policy_model(k1, obs_dim, act_dim, cfg.policy)

    rng, kq1 = jax.random.split(rng)
    q1_net, q1_state = init_q_function(kq1, obs_dim, act_dim, cfg.q_function)
    rng, kq2 = jax.random.split(rng)
    q2_net, q2_state = init_q_function(kq2, obs_dim, act_dim, cfg.q_function)

    # target Q
    _, target_q1_state = init_q_function(kq1, obs_dim, act_dim, cfg.q_function)
    _, target_q2_state = init_q_function(kq2, obs_dim, act_dim, cfg.q_function)

    # dynamics model
    rng, km = jax.random.split(rng)
    model_net, model_state = init_model(km, obs_dim, act_dim, opp_dim, cfg.model_dynamics)

    # opponentï¼ˆå¯é€‰ï¼Œå…ˆä¸è®­ç»ƒä¹Ÿå¯ç”¨ï¼‰
    opponent_states = []
    for _ in range(opp_num):
        rng, ko = jax.random.split(rng)
        _, opp_state = init_policy_model(ko, obs_dim, act_dim, cfg.policy)
        opponent_states.append(opp_state)

    print("âœ… Init done.")

    # ============= è®­ç»ƒå¾ªç¯ =============
    for epoch in tqdm(range(1, cfg.train.epochs + 1), desc="Training Epochs"):
        print(f"\n===== Epoch {epoch}/{cfg.train.epochs} =====")

        # -------------------------------------------------
        # 1) çœŸå®ç¯å¢ƒé‡‡æ · D_env
        # -------------------------------------------------
        policy_fn = make_policy_fn(policy_state)
        opp_fn = make_opp_fn(opponent_states)

        rng, kc = jax.random.split(rng)
        batch_env, final_state, rng = collect_real_data(
            policy_fn=policy_fn,
            opp_fn=opp_fn,
            obs_dim=obs_dim,
            act_dim=act_dim,
            opp_num=opp_num,
            opp_dim=act_dim,   # å¦‚æœæ¯ä¸ªå¯¹æ‰‹ä¸ ego ç»´åº¦ä¸åŒï¼Œè¿™é‡Œæ”¹æˆå¯¹åº”ç»´åº¦
            steps=cfg.collect.steps_per_epoch,
            key=kc,
        )
        # print("Replay obs shape:", replay_env.obs.shape)
        # print("Replay ego act shape:", replay_env.a_ego.shape)
        # print("Replay opponent act shape:", replay_env.a_opp.shape)
        # print("Replay next_obs shape:", replay_env.next_obs.shape)
        # print("Replay rew shape:", replay_env.rew.shape)
        # print("Replay done shape:", replay_env.done.shape)
        # print("Replay current size:", replay_env.size)
        # print("batch obs shape:", batch_env["obs"].shape)
        # print("batch act shape:", batch_env["a_ego"].shape)
        # print("batch next_obs shape:", batch_env["next_obs"].shape)
        # print("batch rew shape:", batch_env["rew"].shape)
        # print("batch done shape:", batch_env.done.shape)
        # print("batch current size:", batch_env.size)
        replay_env = add_batch_to_replay(replay_env, batch_env)
        # print("Replay_env",replay_env)

        # -------------------------------------------------
        # 2) åŸºäº D_env æ‹Ÿåˆ Standardizerï¼Œå¹¶è®­ç»ƒ dynamics
        # -------------------------------------------------
        # ç”¨ä¸€æ‰¹ env æ•°æ®ä¼°è®¡å‡å€¼æ–¹å·®
        rng, ks = jax.random.split(rng)
        boot = replay_env.sample(ks, batch_size=min(cfg.train.batch_size, len(replay_env)))
        std = Standardizer.fit(boot["obs"], boot["a_ego"], boot["a_opp"], boot["next_obs"])

        # è®­ç»ƒ dynamics
        for _ in range(cfg.train.model_updates):
            rng, kb = jax.random.split(rng)
            b = replay_env.sample(kb, batch_size=cfg.train.batch_size)
            model_state, metrics_m = model_train_step(model_state, b, std)
        print(f"Model NLL: {float(metrics_m['nll']):.4f}")

        # -------------------------------------------------
        # 3) æ¨¡å‹ rollout ç”Ÿæˆ D_model
        #    ï¼ˆrollout.py å†…éƒ¨å·²åŒ…å« adaptive n^j é€»è¾‘ï¼‰
        # -------------------------------------------------
        rng, kr = jax.random.split(rng)
        replay_model = rollout_model(
            rng=kr,
            model_state=model_state,
            std=std,
            policy_state=policy_state,
            opponent_policies=[{"state": s} for s in opponent_states],
            replay_env=replay_env,
            replay_model=replay_model,
            cfg=cfg,
        )

        # -------------------------------------------------
        # 4) ç”¨ D_model æ›´æ–° Q & Policy
        # -------------------------------------------------

        for _ in range(cfg.train.gradient_updates):
            rng, ksamp = jax.random.split(rng)
            batch = replay_model.sample(ksamp, batch_size=cfg.train.batch_size)

            # å…ˆæ›´æ–°ä¸¤ä¸ª Qï¼ˆupdate_q_function é‡Œå·²åšæœ€å°åŒ–ç›®æ ‡ï¼‰
            q1_state, q2_state, q_metrics, rng = update_q_function(
                q1_state=q1_state,
                q2_state=q2_state,
                target_q1_state=target_q1_state,
                target_q2_state=target_q2_state,
                policy_state=policy_state,
                batch=batch,
                cfg=cfg.q_function,
                rng=rng,
            )

            q1_pred = q1_state.apply_fn({"params": q1_state.params}, batch["obs"], batch["a_ego"])
            q2_pred = q2_state.apply_fn({"params": q2_state.params}, batch["obs"], batch["a_ego"])
            mean_q1 = jnp.mean(q1_pred)
            mean_q2 = jnp.mean(q2_pred)
            if mean_q1 < mean_q2:
                smaller_q_state = q1_state
                # print("Q1 is smaller, mean:", float(mean_q1))
            else:
                smaller_q_state = q2_state
                # print("Q2 is smaller, mean:", float(mean_q2))

            # å†æ›´æ–° policyï¼ˆç”¨ Q çš„æœ€å°å€¼ï¼‰
            policy_state, pi_metrics = update_policy(
                policy_state=policy_state,
                q_state=smaller_q_state,   # å¦‚æœä½ åœ¨ update_policy é‡Œä½¿ç”¨ min(Q1,Q2)ï¼Œè¿™é‡Œä¼ ä¸ªç»“æ„æˆ–æ”¹å‡½æ•°
                batch=batch,
                cfg=cfg.policy,
                rng=rng,
            )

            # è½¯æ›´æ–° target Q
            target_q1_state = soft_update(target_q1_state, q1_state, cfg.q_function.tau)
            target_q2_state = soft_update(target_q2_state, q2_state, cfg.q_function.tau)

        # ç®€å•æ—¥å¿—
        q1l = float(q_metrics.get("q1_loss", 0.0))
        q2l = float(q_metrics.get("q2_loss", 0.0))
        pil = float(pi_metrics.get("policy_loss", 0.0))
        print(f"[Epoch {epoch}] Q1 {q1l:.4f} | Q2 {q2l:.4f} | Policy {pil:.4f}")

    print("\nğŸ‰ Training finished.")


if __name__ == "__main__":
    # é¿å… Hydra æ”¹å·¥ä½œç›®å½•æ‰¾ä¸åˆ°ç›¸å¯¹è·¯å¾„
    os.environ.setdefault("HYDRA_FULL_ERROR", "1")
    main()
