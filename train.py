# aorpo/train.py
from __future__ import annotations
import os
import jax
import jax.numpy as jnp
import hydra
from omegaconf import DictConfig, OmegaConf
from tqdm import tqdm
from jax.flatten_util import ravel_pytree
import wandb, random
import copy

# ===== ä½ é¡¹ç›®é‡Œçš„æ¨¡å— =====
from aorpo.utils.replay import ReplayBuffer
from aorpo.rollout.collect import collect_real_data, episode_reward
from aorpo.rollout.rollout import rollout_model, compute_rollout_lengths

from aorpo.agents.policy import init_policy_model, PolicyNet
from aorpo.agents.q_function import init_q_function   # ä½ åœ¨ q_function.py é‡Œæä¾›çš„åˆå§‹åŒ–å‡½æ•°
from aorpo.agents.update_q_function import update_q_function
from aorpo.agents.update_policy import update_policy


from aorpo.agents.model_dynamics import (
    init_model,
    train_step as model_train_step,
    Standardizer,
)

wandb.login()
# Start a new wandb run to track this script.
run = wandb.init(
    # Set the wandb entity where your project will be logged (generally your team name).
    entity="yachen-tian-rwth-aachen-university",
    # Set the wandb project where this run will be logged.
    project="AORPO",
    # Track hyperparameters and run metadata.
    config={
        "learning_rate": 3e-4,
        "architecture": "AORPO",
        "Environment": "mpe_spread_v3",
        "epochs": 10,
    },
)



# -------------------------------------------------
# è¾…åŠ©ï¼šè½¯æ›´æ–° target Q
# -------------------------------------------------
def soft_update(target_state, source_state, tau: float):
    new_params = jax.tree_util.tree_map(
        lambda t, s: (1.0 - tau) * t + tau * s, target_state.params, source_state.params
    )
    return target_state.replace(params=new_params)


# -------------------------------------------------
# è¾…åŠ©ï¼šæŠŠä¸€æ‰¹ dict(jnp arrays) åŠ å…¥ replay
# -------------------------------------------------
def add_batch_to_replay(replay: ReplayBuffer, batch: dict, cfg:DictConfig) -> ReplayBuffer:
    return replay.add_batch(batch, cfg)


# -------------------------------------------------
# JAX é£æ ¼ policy / opponent çš„â€œå¯è°ƒç”¨å‡½æ•°â€ï¼ˆä¾› collect ä½¿ç”¨ï¼‰
#   collect_real_data(policy_fn, opp_fn, ...) æœŸæœ›ï¼š
#   - policy_fn(s, key) -> ego åŠ¨ä½œ a_i
#   - opp_fn(s, key)    -> æ‹¼å¥½çš„å¯¹æ‰‹åŠ¨ä½œå‘é‡ a_-i
# -------------------------------------------------
def make_policy_fn(policy_state):
    def policy_fn(obs, key):
        act, _, new_key = PolicyNet.sample_action(
            policy_state.params,
            policy_state.apply_fn,
            key,
            obs["agent_0"],
        )
        return act, new_key
    return policy_fn



def make_opp_fn(opponent_states):
    def opp_fn(obs, key):
        acts = []
        key, sub = jax.random.split(key)
        for i, state in enumerate(opponent_states):
            a_j, _, sub = PolicyNet.sample_action(
                state.params,
                state.apply_fn,
                sub,
                obs[f"agent_{i+1}"]
            )
            acts.append(a_j)
        return jnp.concatenate(acts, -1), sub
    return opp_fn



# -------------------------------------------------
# ä¸»è®­ç»ƒæµç¨‹ï¼ˆHydraï¼‰
# -------------------------------------------------
@hydra.main(config_path="aorpo/configs", config_name="train", version_base=None)
def main(cfg: DictConfig):

    print("\n===== Config =====")
    print(OmegaConf.to_yaml(cfg))

    rng = jax.random.PRNGKey(cfg.seed)

    # ç»´åº¦
    state_dim = cfg.env.state_dim
    num_opponents = cfg.train.num_opponents
    num_agents = num_opponents + 1
    obs_dim = cfg.env.obs_dim
    act_dim = cfg.env.act_dim
    opp_num = getattr(cfg.train, "num_opponents", 0)
    opp_dim = act_dim * opp_num  # ç®€å•å‡è®¾æ¯ä¸ªå¯¹æ‰‹åŠ¨ä½œç»´åº¦ä¸ ego ç›¸åŒ



    # --- Replay Buffers
    replay_env = ReplayBuffer.create(cfg.replay.capacity, obs_dim, act_dim, opp_num, state_dim)
    replay_model = ReplayBuffer.create(cfg.replay.capacity, obs_dim, act_dim, opp_num, state_dim)
    dummy_state = replay_env.state[0]
    _, unravel_fn = ravel_pytree(dummy_state)


    # --- åˆå§‹åŒ–ç½‘ç»œ
    rng, k1 = jax.random.split(rng)
    _, policy_state = init_policy_model(k1, obs_dim, act_dim, cfg.policy, "agent_0")

    rng, kq1 = jax.random.split(rng)
    q1_net, q1_state = init_q_function(kq1, state_dim, act_dim, cfg.q_function)
    rng, kq2 = jax.random.split(rng)
    q2_net, q2_state = init_q_function(kq2, state_dim, act_dim, cfg.q_function)

    # target Q
    _, target_q1_state = init_q_function(kq1, state_dim, act_dim, cfg.q_function)
    _, target_q2_state = init_q_function(kq2, state_dim, act_dim, cfg.q_function)

    # dynamics model
    rng, km = jax.random.split(rng)
    model_net, model_state = init_model(km, state_dim, num_agents, act_dim, opp_dim, cfg.model_dynamics)

    # opponent
    opponent_states = []
    for i in range(opp_num):
        rng, ko = jax.random.split(rng)
        j = i+1
        _, opp_state = init_policy_model(ko, obs_dim, act_dim, cfg.policy, f"agent_{j}")
        opponent_states.append(opp_state)

    # real opponent
    real_opponent_states = []
    for i in range(opp_num):
        rng, ko = jax.random.split(rng)
        _, real_opp_state = init_policy_model(ko, obs_dim, act_dim, cfg.policy, f"agent_{i+1}")
        real_opponent_states.append(real_opp_state)

    print("âœ… Init done.")

    # ============= è®­ç»ƒå¾ªç¯ =============
    for epoch in tqdm(range(1, cfg.train.epochs + 1), desc="Training Epochs"):
        print(f"\n===== Epoch {epoch}/{cfg.train.epochs} =====")

        # -------------------------------------------------
        # 1) çœŸå®ç¯å¢ƒé‡‡æ · D_env
        # -------------------------------------------------
        policy_fn = make_policy_fn(policy_state)
        opp_fn = make_opp_fn(opponent_states)
        real_opp_fn = make_opp_fn(real_opponent_states)

        rng, kc = jax.random.split(rng)
        batch_env, final_state, rng = collect_real_data(
            policy_fn=policy_fn,
            opp_fn=real_opp_fn,
            obs_dim=obs_dim,
            act_dim=act_dim,
            opp_num=opp_num,
            opp_dim=act_dim,   # å¦‚æœæ¯ä¸ªå¯¹æ‰‹ä¸ ego ç»´åº¦ä¸åŒï¼Œè¿™é‡Œæ”¹æˆå¯¹åº”ç»´åº¦
            key=kc,
            cfg=cfg
        )

        replay_env = add_batch_to_replay(replay_env, batch_env, cfg)
        # -------------------------------------------------
        # 2) åŸºäº D_env æ‹Ÿåˆ Standardizerï¼Œå¹¶è®­ç»ƒ dynamics
        # -------------------------------------------------
        # ç”¨ä¸€æ‰¹ env æ•°æ®ä¼°è®¡å‡å€¼æ–¹å·®
        rng, ks = jax.random.split(rng)
        boot = replay_env.sample(ks, batch_size=min(cfg.train.batch_size, len(replay_env)), opp_num=opp_num)
        std = Standardizer.fit(boot["state"], boot["a_ego"], boot["a_opp"], boot["next_state"])

        # è®­ç»ƒ dynamics
        for i in range(cfg.train.model_updates):
            rng, kb = jax.random.split(rng)
            b = replay_env.sample(kb, batch_size=cfg.train.batch_size, opp_num=opp_num)
            model_state, metrics_m = model_train_step(model_state, b, std)
            wandb.log({
                "dynamics_error": metrics_m["nll"],
                "dynamics_mse": metrics_m["mse"],
                "dynamics_logvar": metrics_m["logvar"],
                "model_step": i
            })
        print(f"Model NLL: {float(metrics_m['nll']):.4f}")

        # -------------------------------------------------
        # 3) æ¨¡å‹ rollout ç”Ÿæˆ D_model
        #    ï¼ˆrollout.py å†…éƒ¨å·²åŒ…å« adaptive n^j é€»è¾‘ï¼‰
        # -------------------------------------------------
        rng, kr = jax.random.split(rng)
        replay_model = rollout_model(
            rng=kr,
            model_state=model_state,
            std=std,
            policy_state=policy_state,
            opponent_policies=[{"state": s} for s in opponent_states],
            replay_env=replay_env,
            replay_model=replay_model,
            cfg=cfg,
        )

        # -------------------------------------------------
        # 4) ç”¨ D_model æ›´æ–° Q & Policy
        # -------------------------------------------------

        for i in range(cfg.train.gradient_updates):
            rng, subkey = jax.random.split(rng)
            batch = replay_model.sample(subkey, batch_size=cfg.train.batch_size, opp_num=opp_num)

            # å…ˆæ›´æ–°ä¸¤ä¸ª Qï¼ˆupdate_q_function é‡Œå·²åšæœ€å°åŒ–ç›®æ ‡ï¼‰
            q1_state, q2_state, q_metrics, rng = update_q_function(
                q1_state=q1_state,
                q2_state=q2_state,
                target_q1_state=target_q1_state,
                target_q2_state=target_q2_state,
                policy_state=policy_state,
                opponent_policies=[{"state": s} for s in opponent_states],
                batch=batch,
                cfg=cfg.q_function,
                rng=rng,
            )
            joint_act = jnp.concatenate([batch["a_ego"], batch["a_opp"]], axis=-1)
            q1_pred = q1_state.apply_fn({"params": q1_state.params}, batch["state"], joint_act)
            q2_pred = q2_state.apply_fn({"params": q2_state.params}, batch["state"], joint_act)
            mean_q1 = jnp.mean(q1_pred)
            mean_q2 = jnp.mean(q2_pred)
            std_q1 = jnp.std(q1_pred)
            if mean_q1 < mean_q2:
                smaller_q_state = q1_state
                # print("Q1 is smaller, mean:", float(mean_q1))
            else:
                smaller_q_state = q2_state
                # print("Q2 is smaller, mean:", float(mean_q2))


            # policy_state, pi_metrics = update_policy(
            #     policy_state=policy_state,
            #     q_state=smaller_q_state,   # å¦‚æœä½ åœ¨ update_policy é‡Œä½¿ç”¨ min(Q1,Q2)ï¼Œè¿™é‡Œä¼ ä¸ªç»“æ„æˆ–æ”¹å‡½æ•°
            #     batch=batch,
            #     cfg=cfg.policy,
            #     rng=rng,
            #     opponent_policies=[{"state": s} for s in real_opponent_states],
            # )
            # è½¯æ›´æ–° target Q
            target_q1_state = soft_update(target_q1_state, q1_state, cfg.q_function.tau)
            target_q2_state = soft_update(target_q2_state, q2_state, cfg.q_function.tau)
            wandb.log({
                "q1_loss": q_metrics["q1_loss"],
                "q2_loss": q_metrics["q2_loss"],
                "q1_pred": q_metrics["q1_pred"],
                "q2_pred": q_metrics["q2_pred"],
                # "policy_loss": pi_metrics["policy_loss"],
                "sac_step": i
            })

        update_real_opp_state = []
        for state in real_opponent_states:
            opp_state = state.replace(
                params = copy.deepcopy(policy_state.params),
                opt_state = copy.deepcopy(policy_state.opt_state),
                step = policy_state.step,
            )
            update_real_opp_state.append(opp_state)

        real_opponent_states = update_real_opp_state
        rng, kr = jax.random.split(rng, 2)
        policy_fn = make_policy_fn(policy_state)
        opp_fn = make_opp_fn(real_opponent_states)
        epi_reward = episode_reward(policy_fn, opp_fn, num_agents, kr, cfg)
        wandb.log({
            "episode_reward": epi_reward
        })

        # ç®€å•æ—¥å¿—
        q1l = float(q_metrics.get("q1_loss", 0.0))
        q2l = float(q_metrics.get("q2_loss", 0.0))
        # pil = float(pi_metrics.get("policy_loss", 0.0))
        print(f"[Epoch {epoch}] Q1 {q1l:.4f} | Q2 {q2l:.4f} | Policy") # {pil:.4f}

    wandb.finish()
    print("\nğŸ‰ Training finished.")


if __name__ == "__main__":
    os.environ.setdefault("HYDRA_FULL_ERROR", "1")
    main()